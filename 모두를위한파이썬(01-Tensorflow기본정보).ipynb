{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 01 : 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Andrew Ng's ML class\n",
    "  1. https://class.coursera.org/ml-003/lecture (앤드류 응 교수님꺼 무조건 들으셈)\n",
    "  2. http://www.holehouse.org/mlclass/ (앤드류 응 교수님 수업 필기본)\n",
    "\n",
    "2. Convolutional Neural Networks for Visual Recognition.\n",
    "  1. http://cs23ln.github.io/\n",
    "\n",
    "3. Tensorflow\n",
    "    1. https://www.tensorflow.org\n",
    "    2. https://github.com/aymericdamien/TensorFlow-Examples (텐서 예시들)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 02 : ML 이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning 은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__배경 :__ Limitations of explicit programming : 특정한 조건에 어떤 행동을하라고 구체적인 지시를 일일이 하는 것에는 한계가 있다.\n",
    "\n",
    "ex) 스팸 필터(조건 너무 많음), 자동운전시스템(조건많)\n",
    "\n",
    "__탄생 :__\n",
    "Arthur Samuel (1959) : 머신러닝은 개발자의 그런 구체적인 명령없이 컴퓨터가 스스로 배울 수 있는 능력을 갖는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised/Unsupervise learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning :__ label 이 정해진, 답이 있는 예시들을 가지고 배우는 것\n",
    "\n",
    "ML에서 대부분의 문제 유형들은 Supervised learning 이다.\n",
    "\n",
    "ex : 이미지 라벨링, 스팸메일 구분, 점수 예측.\n",
    "\n",
    "__Types of supervised learning :__ 지도 학습의 종류\n",
    "1. Regression (회귀) ; 실수값을 예측하는 것. 변수 값들에 맞는 곡선이나 직선을 예측하는 것. 오차를 최소한으로 함.\n",
    "    ex) 공부시간에 따른 점수 분포\n",
    "2. Binary classification (이분법 분류) : 딱 두 가지의 종류로 분류하는 것\n",
    "3. Multi-label classification (다중 라벨 분류) ; 여러 가지의 라벨로 분류하는 것.\n",
    "\n",
    "__Unsupervised learning :__ label 이 없고, grouping, clustering 같이 묶는 것 등.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 03 : TensorFlow의 설치 및 기본적인 operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor : 데이터 중에서 3차원 데이터.\n",
    "\n",
    "Data Flow Graph : 노드와 엣지로 연결된 그래프. 데이터는 엣지를 통해 흐른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 operations (TF version 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constant, Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tf.constant(\" \") :__ 텐서플로우에서 쓰는 변수. 해당 값의 노드를 추가한다.\n",
    "\n",
    "__tf.Session() :__ 텐서플로우 1 에서는 session을 만들고 거기에서 run을 하여 print 할 수 있었다.\n",
    "\n",
    "__print(sess.run(변수)) :__ 해당 constant 노드를 실행하여 print 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello, TensorFlow!', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF 1 버전\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node add 노드 더하기\n",
    "\n",
    "node = tf.add(node1, node2)\n",
    "\n",
    "node = node1 + node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.3, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(1.0, tf.float32)\n",
    "node2 = tf.constant(2.3, tf.float32)\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.3, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node4 = node1 + node2\n",
    "\n",
    "print(node4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF 1 버전\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder 노드 : 지정해주는 값을 변수로 가짐.\n",
    "\n",
    "TF2 에서는 placeholder 가 없고 그냥 함수를 정의한다.\n",
    "\n",
    "tf.placeholder( data type ) : run 할 당시에 들어오는 값을 노드로 만든다.\n",
    "\n",
    "feed_dict 가 들어오는 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def adder(a, b):\n",
    "    return a + b\n",
    "    \n",
    "A = tf.constant(2)\n",
    "B = tf.constant(3)\n",
    "\n",
    "print(adder(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF1  example\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_noder = a + b\n",
    "sess.run(adder_node, feed_dict={a: 3, b: 4.5})\n",
    "sess.run(adder_node, feed_dict={a: [1, 3], b: [2, 4]})) #이런 식으로 array 더하기 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 기본정보 (Tensor Ranks, Shapes, and Types)\n",
    "\n",
    "__Tensor Ranks :__ 몇 차원 array냐를 물어보는 것.\n",
    "\n",
    "|랭크|이름|예시|\n",
    "|:-|-|:-|\n",
    "|0랭크|스칼라|(값만 있음) ex 483|\n",
    "|1랭크|벡터|(값과 방향) ex [1, 2, 3]|\n",
    "|2랭크|매트릭스|(수의 표) ex [[1, 2, 3], [2, 3, 4], [5, 6, 7]]|\n",
    "|3랭크|3-Tensor|(수의 큐브) ex [[[1], [2], [3]], [[3], [4], [5]], [[5], [5], [7]]]|\n",
    "|n랭크|n-tensor|ex|\n",
    "\n",
    "\n",
    "__Tensor Shapes :__ 각각의 엘리먼트에 몇 개씩 들어있느냐.\n",
    "\n",
    "|Rank |Shape |Dimension number|\n",
    "|-|-|-|\n",
    "|0랭크 |[]| 0-D|\n",
    "|1랭크 |[D0]| 1-D|\n",
    "|2랭크 |[D0, D1]| 2-D|\n",
    "|3랭크 |[D0, D1, D2]| 3-D|\n",
    "|n랭크 |[D0, D1, D2 ... ]|\n",
    "\n",
    "\n",
    "__Tensor Types__\n",
    "\n",
    "주로 tf.float32 와 tf.int32를 많이 쓴다.\n",
    "\n",
    "\n",
    "__TensorFlow Mechanics__\n",
    "1. Build graph using TensorFlow operations : 텐서플로우 그래프 빌드하기\n",
    "2. sess.run (op) : 그래프 돌리기\n",
    "3. 그래프 변수 업데이트."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 04 : Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear Regression : 주어진 데이터들을 학습하여 실수값을 예측할 수 있는 직선 모델.\n",
    "\n",
    "기본적으로 많은 자연 현상들은 직선을 띈다.\n",
    "\n",
    "__학습 :__ 데이터에 맞는 선(모델)을 찾는 것.\n",
    "\n",
    "__Linear :__ 즉 직선형 모델. $H(x)=Wx+b$\n",
    "\n",
    "*Hypothesis 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떤 모델(가설)이 최적일까?\n",
    "\n",
    "기본적으로 원래 데이터와 예측한 값과의 거리가 멀수록 안좋은 모델이다.\n",
    "\n",
    "__Cost function(Loss function) :__ 예측 값과 원래 데이터와의 거리를 구하는 함수\n",
    "\n",
    "__학습의 목적 :__ 즉 cost 함수를 최소화하는 것. 최적화하는 것이 학습의 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function(loss function) :\n",
    "\n",
    "cost function(loss function) : 예측값과 원래 y 간의 거리 구하는 함수.\n",
    "\n",
    "가장 기본적인 cost function : $H(x)-y$  // 절댓값 처리가 안되어 있음.\n",
    "\n",
    "__가장 자주쓰는 cost function :__ $(H(x)-y)^2$  // 절댓값 처리도 되고, 거리가 멀수록 페널티 줄 수 있음. 오차제곱의 평균\n",
    "\n",
    "cost function : $cost=\\frac{1}{m}\\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^{2}$ 일반적인 데이터에서 cost 함수\n",
    "\n",
    "*m:총 데이터 개수.\n",
    "\n",
    "__cost function(W, b) :__ $cost(W, b)=\\frac{1}{m}\\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^{2}$ cost 함수, 손실 함수는 W, b에 대한 함수가 된다.\n",
    "\n",
    "*$H(x)=Wx+b$이므로 W와 b값이 변화함에 따라서 cost 함수 값이 변화함.\n",
    "\n",
    "__즉 학습의 목적은 cost(W, b)가 가장 작게 나올 때의 W와 b를 구하는 것.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 05 : Linear Regression Tensorflow로 구현 (실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적 : x와 y는 이미 정해져 있다. 정해진 X y에 해당하는 최적의 W b를 찾는 것.\n",
    "\n",
    "1. W, b를 랜덤으로 일단 변수로 잡는다.\n",
    "2. hypothesis = X * W + b\n",
    "3. cost function 을 오차제곱의 평균으로 잡는다.\n",
    "4. cost function 을 줄이는 최적화 방법 : 경사하강법. (learning_rate 설정)\n",
    "    - 여기서 W, b가 계속 업데이트 된다.\n",
    "5. train 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 2 Linear Regression\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(2001):\n",
    "        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val, b_val)\n",
    "\n",
    "'''\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]\n",
    "\"\"\"\n",
    "0 2.82329 [ 2.12867713] [-0.85235667]\n",
    "20 0.190351 [ 1.53392804] [-1.05059612]\n",
    "40 0.151357 [ 1.45725465] [-1.02391243]\n",
    "...\n",
    "1920 1.77484e-05 [ 1.00489295] [-0.01112291]\n",
    "1940 1.61197e-05 [ 1.00466311] [-0.01060018]\n",
    "1960 1.46397e-05 [ 1.004444] [-0.01010205]\n",
    "1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n",
    "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Verision 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sequential()__\n",
    "\n",
    "ex : tf.keras.Sequential()\n",
    "\n",
    "각각 레이어에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택일 때 적합하다.\n",
    "\n",
    "선형 모델, 순차 모델. 들어오는 방향이 한 군데, 나가는 방향도 한 군데.\n",
    "\n",
    "모델의 큰 형태. templete 느낌. 껍데기고, 내부를 채워야 함.\n",
    "\n",
    "input_shape를 모르면 weight 값이 없어서 summary를 볼 수가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dense()__\n",
    "\n",
    "기본적으로 hidden layer 만들 수 있는 layer. 노드 수를 정해줄 수 있다.\n",
    "\n",
    "output : input과 가중치값을 dot하고 + bias를 하여 활성화함수에 넣는다.\n",
    "\n",
    "input rank 가 2보다 크다면 input 과 kernel(가중치)는 dot 계산을 한다.\n",
    "\n",
    "__입력 형태정보 :__ Sequential 모델의 첫번째 레이어는 입력형태 정보 필요.\n",
    "\n",
    "__Dense :__ 2D 레이어 이기 때문에, 입력 형태를 input_dim 으로 넣어줄 수 있다.\n",
    "\n",
    "__input shape ;__ input_shape=(None, 10) 등 정수와 None으로 이루어진 튜플.\n",
    "\n",
    "나머지 레이어들은 input_shape=(12, 2) 등의 튜플 형태로 써야한다. None은 음이 아닌 어떠한 정수를 받을 수 있음.\n",
    "\n",
    "ex : tf.keras.layers.Dense(units=1, input_dim=1)) \n",
    "    - units : output shape\n",
    "    - input_dim : input shape\n",
    "    \n",
    "ex : tf.keras.layers.Dense(10, input_shape=(1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__compile()__ : 학습시키기 전, 학습 방식에 대한 환경설정.\n",
    "\n",
    "1. optimizer(정규화기) : SGD 같은 것.\n",
    "2. loss function(손실 함수) : mse 같은 것. mean_squared_error , $\\frac{1}{m} * \\sum(y'-y)^2$\n",
    "3. metric (기준) : metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.7035\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2692\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6219\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3282\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1935\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1303\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0734\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0670\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0621\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0580\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0544\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0511\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0481\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0452\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0425\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0400\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0377\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0354\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0333\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0314\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0246\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0099\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0082\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0065\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0057\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0054\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0048\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0042\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0028\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0022\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0017\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0014\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0010\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.8106e-04\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2319e-04\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6874e-04\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1750e-04\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6928e-04\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2391e-04\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.8121e-04\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.4103e-04\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0322e-04\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6764e-04\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3416e-04\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0266e-04\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7301e-04\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4511e-04\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1886e-04\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9415e-04\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7090e-04\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4903e-04\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.2844e-04\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.0907e-04\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9084e-04\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.7368e-04\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5754e-04\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.4235e-04\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2806e-04\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1461e-04\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0195e-04\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9004e-04\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7883e-04\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6828e-04\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5836e-04\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4902e-04\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4023e-04\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3196e-04\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2417e-04\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1685e-04\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0996e-04\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0347e-04\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7368e-05\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1625e-05\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6220e-05\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1136e-05\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6350e-05\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.1847e-05\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7610e-05\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3621e-05\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9869e-05\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.6337e-05\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3015e-05\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9888e-05\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6945e-05\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4176e-05\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1571e-05\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9119e-05\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.6812e-05\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4640e-05\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.2598e-05\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0675e-05\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8865e-05\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7163e-05\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5561e-05\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4054e-05\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.2634e-05\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1300e-05\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0043e-05\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8861e-05\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7749e-05\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6702e-05\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5717e-05\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4790e-05\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3918e-05\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3097e-05\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2324e-05\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1597e-05\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0913e-05\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0269e-05\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6639e-06\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.0939e-06\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5576e-06\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 8.0526e-06\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5780e-06\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.1308e-06\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7102e-06\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.3146e-06\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9420e-06\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.5915e-06\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2618e-06\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9512e-06\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6593e-06\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3844e-06\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1259e-06\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8826e-06\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6537e-06\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4381e-06\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2352e-06\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0445e-06\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8649e-06\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6959e-06\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5370e-06\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3873e-06\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2465e-06\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1139e-06\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9894e-06\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8720e-06\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7615e-06\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6577e-06\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5599e-06\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4678e-06\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3814e-06\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2999e-06\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2232e-06\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1511e-06\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0832e-06\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0193e-06\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 9.5915e-07\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0262e-07\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 8.4933e-07\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9930e-07\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5206e-07\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.0779e-07\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6596e-07\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2672e-07\n",
      "[[-3.9986835]\n",
      " [-2.9993227]]\n"
     ]
    }
   ],
   "source": [
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# units == output shape, input_dim == input shape\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.1)  # SGD == standard gradient descendent, lr == learning rate\n",
    "tf.model.compile(loss='mse', optimizer=sgd)  # mse == mean_squared_error, 1/m * sig (y'-y)^2\n",
    "\n",
    "# prints summary of the model to the terminal\n",
    "tf.model.summary()\n",
    "\n",
    "# fit() executes training\n",
    "tf.model.fit(x_train, y_train, epochs=200)\n",
    "\n",
    "\n",
    "# predict() returns predicted value\n",
    "y_predict = tf.model.predict(np.array([5, 4]))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 06 : Linear Regression의 cost 최소화\n",
    "## SGD 원리 (Standard Gradient Descendent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설 간단화 Simplified hypothesis\n",
    "\n",
    "$H(x) = Wx$ 계산을 간단히 보여주기 위해서 bias 를 없앤다. 그렇게 되면 cost function은\n",
    "\n",
    "$cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$\n",
    "\n",
    "cost(W)함수는 W의 변화에 따른 Cost 값을 보여준다.\n",
    "\n",
    "이 함수는 이차함수 형태를 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm (경사하강법)  : cost 함수를 최소화하자.\n",
    "\n",
    "- loss 함수를 최소화하다\n",
    "- 대부분의 최소화 문제에서 경사 하강법을 사용한다\n",
    "- cost(loss)를 최소화 하기위한 W, b를 찾는다.\n",
    "- 실생활에서는 함수의 극값을 찾을 수 없기 때문에 사용하는 방법.\n",
    "\n",
    "__작동원리__\n",
    "\n",
    "- 랜덤 초기값( 혹은 특정한 초기값 )에서 시작한다.\n",
    "- W, b 에 따른 경사값이 0이 되는 방향으로 계속 진행한다\n",
    "- 어디서 시작하는지가 어느 최솟값으로 끝나는지를 결정한다.\n",
    "\n",
    "__Convex function__\n",
    "\n",
    "- SGD는 가끔 시작점의 변화(W, b)에 따라 최솟값이 아닌 local 최솟값에 수렴할 수 있다. (제대로 동작 X)\n",
    "- 우리의 현재 cost(W, b) 함수는 밥그릇 뒤집어 놓은 Convex function 이다.\n",
    "- Convex function의 경우 SGD를 사용하면 무조건 답을 찾을 수 있다.\n",
    "- Convex function 인지 아닌지 확인 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 수식 표현\n",
    "\n",
    "1. $cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ 를 2로 나눠 계산을 편하게 한다. (결과에는 지장 없음)\n",
    "2. $cost(W) = \\frac{1}{2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ cost함수를 W에 대해 편미분 한다.\n",
    "3. $W :=W - \\alpha\\frac{\\partial}{\\partial W}cost(W)$ W에 대한 cost 함수 이므로, W에 대해 편미분한다. 그리고 W를 업데이트한다.\n",
    "4. $W :=W - \\alpha\\frac{\\partial}{\\partial W}\\frac{1}{2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ 편미분하는 과정\n",
    "5. $W :=W - \\alpha\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$ W에 대한 곱미분이다.\n",
    "\n",
    "SGD 정리 : $W :=W - \\alpha\\frac{\\partial}{\\partial W}\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$\n",
    "\n",
    "여기서 $\\alpha$는 learning rate 이다. 얼마나 학습할 것인지를 정한다. 현재 기울기를 얼마나 반영하여 뺄 것인가.\n",
    "\n",
    "여기서 $\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$ 이 부분은 현재 W에 의한 cost 값의 기울기이다.\n",
    "\n",
    "따라서 현재 기울기가 양수라면 값이 줄어들어 0에 가까워질 것이고, 현재 기울기가 음수라면 값이 늘어가 0에 가까워질 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 07 : Linear Regression의 cost 최소화 구현 (실습)\n",
    "## SGD (Standard Gradient Descendent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SGD 없이 W 변화에 따른 cost 함수 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_history = []\n",
    "cost_history = []\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    for i in range(-30, 50):\n",
    "        curr_W = i * 0.1\n",
    "        curr_cost = sess.run(cost, feed_dict={W: curr_W})\n",
    "\n",
    "        W_history.append(curr_W)\n",
    "        cost_history.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_history, cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 미분을 이용하여 직접 SGD 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 버전. 직접 구현\n",
    "\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B 버전. optimizer 사용\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 A 버전과 B 버전은 같은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data\n",
    "# We know that W should be 1\n",
    "# But let's use TensorFlow to figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "#### cost 함수와 SGD 구현\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "######################\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(21):\n",
    "        _, cost_val, W_val = sess.run(\n",
    "            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, cost_val, W_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SGD 로 얻은 gradients 확보. (알아서 재 가공후 적용 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer로 GradientDescentOptimizer 사용 후, gradient를 다음과 같이 얻을 수 있다.\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "\n",
    "apply_gradients = optimizer.apply_gradients(gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n",
    "        print(step, gradient_val, gvs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.model.fit(x_train, y_train, epochs= ).history 하면 loss 내역을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3681\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2391\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1769\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1448\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1265\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0989\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0926\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0870\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0818\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0769\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0724\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0641\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0567\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0534\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0502\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0473\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0445\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0419\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0394\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0371\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0349\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0328\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0309\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0291\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0242\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0158\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0132\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0117\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0072\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0031\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0029\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0018\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6600e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0902e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5541e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0495e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5748e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1280e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7075e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3119e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9396e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5893e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2596e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9494e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6575e-04\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3828e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1243e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8810e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6521e-04\n",
      "[[-3.9682183]\n",
      " [-2.983647 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAApAUlEQVR4nO3de5xddX3u8c+z99wvCclkQsiFJIQQCBeJjFxEESliFAV6ihXUChbLgUrRWqtgrSItrdLWIhZ7oEotVkQL4kk9WBSMIKVABojEADEhBDIhIZPrJJnM/Xv+2GuSnWEHZpJZ2ZPZz/v12q+91m+ttfd3uXGe/H7rpojAzMxsoEyxCzAzs5HJAWFmZgU5IMzMrCAHhJmZFeSAMDOzghwQZmZWkAPCbD9ImiEpJJUNYt1LJT2yv59jdqA4IKxkSFolqUvShAHtTyd/nGcUqTSzEckBYaXmReDi/hlJxwM1xSvHbORyQFip+S7w0bz5S4A78leQNFbSHZJaJb0k6QuSMsmyrKS/l7RB0krg3ALbflvSWklrJP21pOxQi5Q0WdICSZskrZD0R3nLTpbULKlN0quSvpa0V0n6d0kbJW2RtEjSoUP9brN+DggrNY8BYyQdk/zhvgj49wHrfAMYCxwBvINcoHwsWfZHwPuAeUATcOGAbb8D9ABHJuucA3x8H+q8C2gBJiff8TeSzkqWfR34ekSMAWYBP0zaL0nqngY0AFcAO/fhu80AB4SVpv5exLuA54A1/QvyQuPaiNgWEauAfwD+IFnl94GbImJ1RGwC/jZv20OB9wKfiogdEbEe+Mfk8wZN0jTgdOBzEdEREYuBb7G759MNHClpQkRsj4jH8tobgCMjojcinoyItqF8t1k+B4SVou8CHwIuZcDwEjABKAdeymt7CZiSTE8GVg9Y1m96su3aZIhnC3ArMHGI9U0GNkXEtr3UcBlwFPB8Moz0vrz9uh+4S9Irkm6UVD7E7zbbxQFhJSciXiJ3sPq9wI8GLN5A7l/i0/PaDmd3L2MtuSGc/GX9VgOdwISIOCR5jYmIY4dY4ivAeEn1hWqIiOURcTG54PkqcLek2ojojogvR8Rc4K3khsI+itk+ckBYqboMOCsiduQ3RkQvuTH9GyTVS5oOfJrdxyl+CFwtaaqkccA1eduuBX4G/IOkMZIykmZJesdQCouI1cCjwN8mB55PSOr9dwBJH5HUGBF9wJZksz5J75R0fDJM1kYu6PqG8t1m+RwQVpIi4oWIaN7L4j8BdgArgUeAO4Hbk2X/Qm4Y59fAU7y2B/JRoAJ4FtgM3A0ctg8lXgzMINebuBf4UkQ8kCybDyyVtJ3cAeuLImInMCn5vjZyx1YeIjfsZLZP5AcGmZlZIe5BmJlZQQ4IMzMryAFhZmYFOSDMzKygUXNr4QkTJsSMGTOKXYaZ2UHlySef3BARjYWWjZqAmDFjBs3Neztr0czMCpH00t6WeYjJzMwKckCYmVlBDggzMyto1ByDKKS7u5uWlhY6OjqKXUrqqqqqmDp1KuXlvnmnmQ2PUR0QLS0t1NfXM2PGDCQVu5zURAQbN26kpaWFmTNnFrscMxslRvUQU0dHBw0NDaM6HAAk0dDQUBI9JTM7cEZ1QACjPhz6lcp+mtmBM+oD4o309gXr2jpo7+opdilmZiNKyQdERLC+rYP2rt5h/+yNGzdy4okncuKJJzJp0iSmTJmya76rq+t1t21ububqq68e9prMzAZrVB+kHoxMJjc009c3/M/FaGhoYPHixQBcd9111NXV8ZnPfGbX8p6eHsrKCv8ETU1NNDU1DXtNZmaDVfI9CAFCpJAPBV166aVcccUVnHLKKXz2s5/liSee4LTTTmPevHm89a1vZdmyZQD88pe/5H3vyz2L/rrrruMP//APOfPMMzniiCO4+eabD0yxZlbSSqYH8eX/XMqzr7QVXNbe1UNZJkNF2dDycu7kMXzp/UN9Hn3u9NtHH32UbDZLW1sbv/rVrygrK+OBBx7g85//PPfcc89rtnn++edZuHAh27ZtY86cOVx55ZW+5sHMUlUyAfH6xIF88OoHPvABstksAFu3buWSSy5h+fLlSKK7u7vgNueeey6VlZVUVlYyceJEXn31VaZOnXoAqzazUlMyAfF6/9Jftm4b1eVZDm+oOSC11NbW7pr+y7/8S975zndy7733smrVKs4888yC21RWVu6azmaz9PT4rCszS1fJH4MAyAj64kD2IXbbunUrU6ZMAeA73/lOUWowMysk1YCQNF/SMkkrJF1TYPkVkpZIWizpEUlzk/YZknYm7Ysl/Z8068xIRQuIz372s1x77bXMmzfPvQIzG1EUKf1hlJQFfgu8C2gBFgEXR8SzeeuMiYi2ZPo84I8jYr6kGcBPIuK4wX5fU1NTDHxg0HPPPccxxxzzhtu+uGEHvX3BkRPrBvt1I9Jg99fMrJ+kJyOi4Dn1afYgTgZWRMTKiOgC7gLOz1+hPxwStXBAjxXvUswhJjOzkSrNgJgCrM6bb0na9iDpE5JeAG4E8i8dninpaUkPSXp7oS+QdLmkZknNra2t+1xoRkrlQjkzs4NZ0Q9SR8QtETEL+BzwhaR5LXB4RMwDPg3cKWlMgW1vi4imiGhqbCz4zG0GM4SW60Hs6x6MDGkNFZpZ6UozINYA0/LmpyZte3MXcAFARHRGxMZk+kngBeCooRZQVVXFxo0b3/CPZyZTvIPUw6H/eRBVVVXFLsXMRpE0r4NYBMyWNJNcMFwEfCh/BUmzI2J5MnsusDxpbwQ2RUSvpCOA2cDKoRYwdepUWlpaeKPhp7aObtp29pDZWs3Betfs/ifKmZkNl9QCIiJ6JF0F3A9kgdsjYqmk64HmiFgAXCXpbKAb2Axckmx+BnC9pG6gD7giIjYNtYby8vJBPWHttodf4G/ue57ffPnd1FWWzLWDZmavK9W/hhFxH3DfgLYv5k1/ci/b3QO89oZEKamuyP3P0N7V44AwM0sU/SD1SFBbkbsv0s4UnglhZnawckAANUlA7Oh0QJiZ9XNAsHuIaWe3b3VhZtbPAcHuIaY0HjtqZnawckAA1R5iMjN7DQcEUOMhJjOz13BAsHuIyT0IM7PdHBDsHmLyaa5mZrs5INg9xOSD1GZmuzkggGxGVJZlaO/yMQgzs34OiERNRdY9CDOzPA6IRE1FmQPCzCyPAyKR60F4iMnMrJ8DIuEhJjOzPTkgEtUVWZ/mamaWxwGRqK0oY4eHmMzMdnFAJNyDMDPbkwMiUVORdQ/CzCxPqgEhab6kZZJWSLqmwPIrJC2RtFjSI5Lm5i27NtlumaR3p1kn+DRXM7OBUgsISVngFuA9wFzg4vwASNwZEcdHxInAjcDXkm3nAhcBxwLzgW8mn5eammSIKSLS/Bozs4NGmj2Ik4EVEbEyIrqAu4Dz81eIiLa82Vqg/6/z+cBdEdEZES8CK5LPS01NRZaevqCrty/NrzEzO2ikGRBTgNV58y1J2x4kfULSC+R6EFcPcdvLJTVLam5tbd2vYnc9E8LDTGZmwAg4SB0Rt0TELOBzwBeGuO1tEdEUEU2NjY37VUeNHztqZraHNANiDTAtb35q0rY3dwEX7OO2+616V0D4TCYzM0g3IBYBsyXNlFRB7qDzgvwVJM3Omz0XWJ5MLwAuklQpaSYwG3gixVqp9TMhzMz2UJbWB0dEj6SrgPuBLHB7RCyVdD3QHBELgKsknQ10A5uBS5Jtl0r6IfAs0AN8IiJS/cvtISYzsz2lFhAAEXEfcN+Ati/mTX/ydba9Abghver2VFPZ34PwEJOZGYyAg9QjhXsQZmZ7ckAkqssdEGZm+RwQidr+IaZODzGZmYEDYpddQ0zd7kGYmYEDYpfKsgwStHc6IMzMwAGxiyRqfUdXM7NdHBB5qiuy7Oz2MQgzM3BA7KGmIssODzGZmQEOiD34oUFmZrs5IPLUeIjJzGwXB0QeDzGZme3mgMjT/9hRMzNzQOyhpqKMdg8xmZkBDog9VFdkfaGcmVnCAZGntiLrs5jMzBIOiDzVFWXs7O6lry+KXYqZWdE5IPL037Bvp2/YZ2bmgMhX64cGmZntkmpASJovaZmkFZKuKbD805KelfSMpAclTc9b1itpcfJakGad/aor/NhRM7N+qT2TWlIWuAV4F9ACLJK0ICKezVvtaaApItolXQncCHwwWbYzIk5Mq75C/NhRM7Pd0uxBnAysiIiVEdEF3AWcn79CRCyMiPZk9jFgaor1vCEHhJnZbmkGxBRgdd58S9K2N5cBP82br5LULOkxSRcU2kDS5ck6za2trftdcI2HmMzMdkltiGkoJH0EaALekdc8PSLWSDoC+IWkJRHxQv52EXEbcBtAU1PTfp+b6h6EmdluafYg1gDT8uanJm17kHQ28BfAeRHR2d8eEWuS95XAL4F5KdYK5J3m6oAwM0s1IBYBsyXNlFQBXATscTaSpHnAreTCYX1e+zhJlcn0BOB0IP/gdir6h5h2eIjJzCy9IaaI6JF0FXA/kAVuj4ilkq4HmiNiAfB3QB3wH5IAXo6I84BjgFsl9ZELsa8MOPspFdXuQZiZ7ZLqMYiIuA+4b0DbF/Omz97Ldo8Cx6dZWyH9F8pt63APwszMV1LnKctmGFtdzqYdXcUuxcys6BwQA0yoq2DD9s43XtHMbJRzQAzQWF9J6zYHhJmZA2KACXWV7kGYmeGAeA33IMzMchwQA0yoq2RHV69vt2FmJc8BMUBjfSUAG7b5TCYzK20OiAH6A6LVxyHMrMQ5IAZorEsCwschzKzEOSAG2DXE5B6EmZU4B8QA42srAPcgzMwcEAOUZzOMr/XV1GZmDogCJtRVuAdhZiXPAVFAY72vpjYzc0AUMKGu0qe5mlnJc0AU0FhXyYZtXUTs92OuzcwOWg6IAibUV7Kzu5cdfrKcmZWwVANC0nxJyyStkHRNgeWflvSspGckPShpet6ySyQtT16XpFnnQP0Xy23wgWozK2GpBYSkLHAL8B5gLnCxpLkDVnsaaIqIE4C7gRuTbccDXwJOAU4GviRpXFq1DjTBt9swM0u1B3EysCIiVkZEF3AXcH7+ChGxMCLak9nHgKnJ9LuBn0fEpojYDPwcmJ9irXtwD8LMLN2AmAKszptvSdr25jLgp/u47bDyDfvMzAYZEJJqJWWS6aMknSepfLiKkPQRoAn4uyFud7mkZknNra2tw1UO42sryMg9CDMrbYPtQTwMVEmaAvwM+APgO2+wzRpgWt781KRtD5LOBv4COC8iOoeybUTcFhFNEdHU2Ng4yF15Y9mMGF/rayHMrLQNNiCUHCv4X8A3I+IDwLFvsM0iYLakmZIqgIuABXt8qDQPuJVcOKzPW3Q/cI6kccnB6XOStgPGt9sws1JXNsj1JOk04MPkjhUAZF9vg4jokXQVuT/sWeD2iFgq6XqgOSIWkBtSqgP+QxLAyxFxXkRskvRX5EIG4PqI2DSkPdtPjfWVtG73U+XMrHQNNiA+BVwL3Jv8kT8CWPhGG0XEfcB9A9q+mDd99utseztw+yDrG3aNdZWsbN1RrK83Myu6QQVERDwEPASQHKzeEBFXp1lYseV6EJ1EBEnvxsyspAz2LKY7JY2RVAv8BnhW0p+nW1pxTairpKunj7aOnmKXYmZWFIM9SD03ItqAC8hdqzCT3JlMo5YfPWpmpW6wAVGeXPdwAbAgIrqBUX2r0wnJ1dQ+k8nMStVgA+JWYBVQCzyc3FSvLa2iRgL3IMys1A32IPXNwM15TS9Jemc6JY0ME+oqAPcgzKx0DfYg9VhJX+u/rYWkfyDXmxi1xtVUkM3IAWFmJWuwQ0y3A9uA309ebcC/plXUSJDJiMmHVPHSpvY3XtnMbBQa7IVysyLi9/LmvyxpcQr1jChHTxrDsnXbil2GmVlRDLYHsVPS2/pnJJ0O7EynpJHjmEn1rGzdTke3Hz1qZqVnsD2IK4A7JI1N5jcDB/QxoMUwZ9IY+gJWrN/OcVPGvvEGZmajyKB6EBHx64h4E3ACcEJEzAPOSrWyEeDow+oBeN7DTGZWgob0RLmIaEuuqAb4dAr1jCgzGmqpLMvw/NpRfcmHmVlB+/PI0VF/B7tsRhx1aL17EGZWkvYnIEb1rTb6HT3JAWFmpel1A0LSNkltBV7bgMkHqMaimjOpng3bO33BnJmVnNc9iyki6g9UISPVMYeNAWDZum277s9kZlYK9meIqSQcPan/TCYfqDaz0pJqQEiaL2mZpBWSrimw/AxJT0nqkXThgGW9khYnrwVp1vl6GuoqmVBX6eMQZlZyBnuh3JBJygK3AO8CWoBFkhZExLN5q70MXAp8psBH7IyIE9OqbyiOOazePQgzKzlp9iBOBlZExMqI6ALuAs7PXyEiVkXEM0BfinXst6Mn1bP81e309I7oMs3MhlWaATEFWJ0335K0DVZVcmvxxyRdUGgFSZf334K8tbV1P0p9fXMmjaGzp49VG31nVzMrHSP5IPX0iGgCPgTcJGnWwBUi4raIaIqIpsbGxtQK8YFqMytFaQbEGmBa3vzUpG1QImJN8r4S+CUwbziLG4ojJ9aRzci3/jazkpJmQCwCZkuaKakCuAgY1NlIksZJqkymJwCnA8++/lbpqSrPMntiHU+8uKlYJZiZHXCpBURE9ABXAfcDzwE/jIilkq6XdB6ApLdIagE+ANwqaWmy+TFAs6RfAwuBrww4++mAO/uYQ1m0ahObd3QVswwzswMmtdNcASLiPuC+AW1fzJteRG7oaeB2jwLHp1nbUJ1z7KH808IVPPj8ei486TUlm5mNOiP5IPWIcvyUsRw2toqfLV1X7FLMzA4IB8QgSeKcuYfy8PJWdnb5EaRmNvo5IIbgnGMn0dHdx8PL07vmwsxspHBADMHJM8cztrqcny19tdilmJmlzgExBOXZDL9z9EQefP5V33bDzEY9B8QQnXPsoWxp72bRqs3FLsXMLFUOiCE646hGKssy3O+zmcxslHNADFFNRRlnzz2Ue55sYevO7mKXY2aWGgfEPrjyHbPY1tnDd/9nVbFLMTNLjQNiHxw3ZSzvnNPItx95kR2dPcUux8wsFQ6IfXTVWbPZ3N7NnY+/XOxSzMxS4YDYRydNH8dbZzVw269W0tHtK6vNbPRxQOyHq846ktZtnfxH8+o3XtnM7CDjgNgPpx3RQNP0cXz9wRVsafdtwM1sdHFA7AdJfPn8Y9nS3sX1Pynq4yrMzIadA2I/HTt5LFeeOYsfPbWGhcvWF7scM7Nh44AYBleddSSzJ9bx+R8tYVuHL54zs9HBATEMKsuy3HjhCbza1sHf3PdcscsxMxsWqQaEpPmSlklaIemaAsvPkPSUpB5JFw5Ydomk5cnrkjTrHA7zDh/H5WfM4vtPrOaeJ1uKXY6Z2X5LLSAkZYFbgPcAc4GLJc0dsNrLwKXAnQO2HQ98CTgFOBn4kqRxadU6XD5zzlG8dVYD1967hGdathS7HDOz/ZJmD+JkYEVErIyILuAu4Pz8FSJiVUQ8Awx8uMK7gZ9HxKaI2Az8HJifYq3Doiyb4Z8+9GYa6yr53999ktZtncUuycxsn6UZEFOA/CvIWpK2YdtW0uWSmiU1t7aOjMeAjq+t4NY/OInN7V388fee9FXWZnbQOqgPUkfEbRHRFBFNjY2NxS5nl+OmjOXvP/AmFq3azNXff9pPnzOzg1KaAbEGmJY3PzVpS3vbEeF9J0zmuvfP5WfPvsrn711CRBS7JDOzIUkzIBYBsyXNlFQBXAQsGOS29wPnSBqXHJw+J2k7qFx6+kyuPutIftjcwld++rxDwswOKmVpfXBE9Ei6itwf9ixwe0QslXQ90BwRCyS9BbgXGAe8X9KXI+LYiNgk6a/IhQzA9RGxKa1a0/Sn7zqKTe1d3PrwSsqy4jPnzEFSscsyM3tDqQUEQETcB9w3oO2LedOLyA0fFdr2duD2NOs7ECRx/XnH0dsX3LLwBXr6gmvmH+2QMLMRL9WAsJxMRtxwwfFkM+LWh1bS2xv8xbnHOCTMbERzQBwgmYz4q/OPIyvxrUdeZFtHDzf87nGUZQ/qE8nMbBRzQBxAkrjuvGMZU13ON36xgk3tXXzj4nlUlWeLXZqZ2Wv4n68HmCT+7Jw5XPf+uTzw3Kt89PYn2NruO8Ca2cjjgCiSS0+fyc0XzWPxy1v43X/+b1Zt2FHskszM9uCAKKL3v2ky3/ujU9i8o4sLvvnfPPHiQXkmr5mNUg6IInvLjPH8+BOnM762gg9/6zG+/8TLxS7JzAxwQIwI0xtquffK0zn1iAau/dESrv3REjp7fJM/MysuB8QIMbamnO987GT++MxZfP+Jl7notsdYu3VnscsysxLmgBhBshnx2flH880Pv5nfrtvGe7/+KxYuW1/sssysRDkgRqD3Hn8YC/7kbRw6poqP/esi/vanz9HtW4ab2QHmgBihZjXW8eNPnM6HTjmcWx9ayYX//CgrW7cXuywzKyEOiBGsqjzL3/zu8Xzzw29m1cZ2zr35Ee58/GXfNtzMDggHxEHgvccfxv2fOoOTpo/j8/cu4WPfWeQD2GaWOgfEQWLS2Cru+MOTue79c3l85SbO+drD/GCRexNmlh4HxEEkkxGXnj6T//rU2zl2yhg+d88SPvLtx31swsxS4YA4CE1vqOXOj5/KX19wHM+0bGX+Tb/ipgd+S0e3L64zs+HjgDhIZTLiI6dO58E/ewfzj5vETQ8s5903PcwDz77qYSczGxapBoSk+ZKWSVoh6ZoCyysl/SBZ/rikGUn7DEk7JS1OXv8nzToPZhPrq7j54nl897KTKcuIj9/RzKX/uogV6z3sZGb7J7WAkJQFbgHeA8wFLpY0d8BqlwGbI+JI4B+Br+YteyEiTkxeV6RV52jx9tmN/NenzuAL5x7DUy9t5t03PcwXfryEDds7i12amR2k0uxBnAysiIiVEdEF3AWcP2Cd84F/S6bvBn5HflDzPivPZvj4249g4Z+fyYdPOZzvP7Gad9y4kJsfXM72zp5il2dmB5k0A2IKsDpvviVpK7hORPQAW4GGZNlMSU9LekjS2wt9gaTLJTVLam5tbR3e6g9iE+oquf784/jZn57B22ZP4Gs//y1n3LiQf3l4pQ9km9mgjdSD1GuBwyNiHvBp4E5JYwauFBG3RURTRDQ1NjYe8CJHulmNddz6B038+BOnc+zkMdxw33O8PQmK9i73KMzs9aUZEGuAaXnzU5O2gutIKgPGAhsjojMiNgJExJPAC8BRKdY6qp047RC+e9kp/ODyUznq0DpuuO853vbVhdyycIWfh21me5VmQCwCZkuaKakCuAhYMGCdBcAlyfSFwC8iIiQ1Jge5kXQEMBtYmWKtJeGUIxr43sdP5Z4rT+OEqWP5u/uX8davPMhf/eRZ1mzxrTvMbE9K85x5Se8FbgKywO0RcYOk64HmiFggqQr4LjAP2ARcFBErJf0ecD3QDfQBX4qI/3y972pqaorm5ubU9mU0evaVNm57+AX+85m1AMw/dhIfO30GJ00fh88VMCsNkp6MiKaCy0bLRVUOiH23ZstO7nh0Fd9/4mXaOno4bsoYPnLKdM47cTI1FWXFLs/MUuSAsEFp7+rh3qfXcMejL7Hs1W3UV5ZxwbwpfPAt0zhuythil2dmKXBA2JBEBE+9vJl/f+xl/t+StXT19DH3sDF8oGkq571pMg11lcUu0cyGiQPC9tnW9m4W/HoNP2xuYcmarWQz4h1HNXLBvCmcfcxED0GZHeQcEDYsnl/Xxo+ffoX/u3gNa7d2UF2e5axjJvK+4w/jzDkTqa7IFrtEMxsiB4QNq76+4IlVm/jJM6/w0yXr2Liji6ryDO84qpH5x03inXMmckhNRbHLNLNBcEBYanp6+3j8xU3cv3Qd9y9dx6ttnWQz4qTp4zj7mIm8c85EjpxY59NmzUYoB4QdEH19weKWLfziufU88NyrPL9uGwBTDqnmjKMaOWP2BE6b1eDehdkI4oCwolizZScP/7aVh5a18t8rNrCtswcJjp8yltNmNXDqEQ28ZcZ46ip9oNusWBwQVnTdvX0807KFR5Zv5JEVrSxevYXu3iCbEcdOHsNbZoznLTPGcdL08TTW+zRaswPFAWEjzs6uXp56eTOPrdzIEy9uYvHqLXT29AEwbXw186aNY97hh/CmaYcw97AxVJX7DCmzNLxeQLhvb0VRXZHl9CMncPqREwDo6uljyZqtPPXSZp56eTOPv7iRBb9+BYCyjJgzqZ7jJo/luCljmDt5LEdPqqfWQ1NmqfL/w2xEqCjLcNL0cZw0fRyQu5p7XVsHv169lWdatrBkzVZ+9uw6ftCcewaVBDMaajnmsHrmHDqGOZPqOOrQeg4fX0NZdqQ+5sTs4OKAsBFJEoeNreawsdXMP24SkAuNV7Z2sHTNVp5bu43n1rax9JU2fvqbdfSPlFZkMxzRWMusiXXMaqxjVmMtsxrrmDGh1gfDzYbI/4+xg4YkphxSzZRDqjnn2Em72tu7elixfjvPr9vGC+u3s3z9dp5p2cJ9S9aSf4itsb6SmQ21HN5Qw/TxNRzeUMO08TVMG1fDhLoKX6thNoADwg56NRVlnDD1EE6Yesge7R3dvby0sZ0XWrfz4oYdrNqwg1Ubd/Cr5a3c3da5x7pV5Zlc+IyrSUKoKteDSd4njanyrUSs5DggbNSqKs8yZ1I9cybVv2bZzq5eVm9uZ/Wm5LV5J2s27+SVrTtZumYrG3d0vWabsdXlTBpTxcQxlTTWVzKxvoqJ9bnpxvpKJtRV0lhXyZjqMvdGbFRwQFhJqq7IctSh9Rx16GvDA3K9j7VbO1i7ZSfr2jpYu7WDV9v6X528sH4767d10tP32tPEy7NiXE0FDXWVNNRWMK62gvE15YyrrWBcTQWH1JTvej+kuoKxNeXUV5aRyThUbGRxQJgVUFWeZeaEWmZOqN3rOn19wdad3bRu76R1WycbkveNO7rYtL2LjTty0y2b29m0o4u2jp69fpYE9ZVljK0pZ0xV8qouo76qnPqq5L2yjLqqMuoqc6/ayjJqK7O7pyvKqCrPuPdiwybVgJA0H/g6uWdSfysivjJgeSVwB3ASsBH4YESsSpZdC1wG9AJXR8T9adZqNlSZjHK9gtqKvfZE8nX39rF1Zzdb2rvY3N7N1vZutiTzbTu72Zq8tnX00NbRzYsbdrC9o4dtHT1s7+phMNe0SlBbUUZ1RZbaiizVFWXUVGSpqchSVZ57ry7PTedeGarK+9ty05VlGSr738ty71XlGSqyWSrKMlSWZahIXmUZOZBGsdQCQlIWuAV4F9ACLJK0ICKezVvtMmBzRBwp6SLgq8AHJc0FLgKOBSYDD0g6KiJ606rXLG3l2QwT6nLHKoaqry9o7+5NAqOb7Z09bO/sYUdnDzs6e9nRlZvf2dXLjs5e2rt6aO/qpb2rl53duWWt2zrp6M61dXT30tHTR1dy9fq+knKnFldkd4dGeTZDeVaUZ3fPl2W0K1DK8paXZXLTZVlRltm9PPcuyjIim7Rnk7ZsZs/2TEZklWvPvSCj3OdlMuxalr9eZtd7Lugzyi3LJNtmBk7nrZdRrl3ac/loDMo0exAnAysiYiWApLuA84H8gDgfuC6Zvhv4J+X+Vz4fuCsiOoEXJa1IPu9/UqzXbMTKZLRraGnS2Kph+9zevqCzp5eO7j52dvfS2Z2b7ujppaunj47uXjqTIOns6aMzad/16t29rKdvd3t3X9CdLO/pDbp6+9je2UNPb9Dd20dPX/KeN9+z6z3o6eujwOGdEW93aIDIhUh+kAhgQMCIXLjk5vO2y2tXofbkswTMnTyWb1w8b9j3J82AmAKszptvAU7Z2zoR0SNpK9CQtD82YNspA79A0uXA5QCHH374sBVuViqyGVFTUcZIvAN7X1/Q0xf0RewKkN6+oLcv6O6LXct7+/ro7SMXKn3QG7Frvd5k+96+oDdy2/S39QV500FvH/RFEAOm89eLyH1+RG55X18Q7Dmdv06QvCef0/8ZsHu6vxbYPZ/bNjdNMh2x+/Pz5wk4fHx1Kr/BQX2QOiJuA26D3M36ilyOmQ2jTEZU+MyuokrzpjVrgGl581OTtoLrSCoDxpI7WD2Ybc3MLEVpBsQiYLakmZIqyB10XjBgnQXAJcn0hcAvInf/8QXARZIqJc0EZgNPpFirmZkNkNoQU3JM4SrgfnKnud4eEUslXQ80R8QC4NvAd5OD0JvIhQjJej8kd0C7B/iEz2AyMzuw/MAgM7MS9noPDPKN883MrCAHhJmZFeSAMDOzghwQZmZW0Kg5SC2pFXhpPz5iArBhmMo5WJTiPkNp7ncp7jOU5n4PdZ+nR0RjoQWjJiD2l6TmvR3JH61KcZ+hNPe7FPcZSnO/h3OfPcRkZmYFOSDMzKwgB8RutxW7gCIoxX2G0tzvUtxnKM39HrZ99jEIMzMryD0IMzMryAFhZmYFlXxASJovaZmkFZKuKXY9aZE0TdJCSc9KWirpk0n7eEk/l7Q8eR9X7FqHm6SspKcl/SSZnynp8eQ3/0FyO/pRRdIhku6W9Lyk5ySdNtp/a0l/mvy3/RtJ35dUNRp/a0m3S1ov6Td5bQV/W+XcnOz/M5LePJTvKumAkJQFbgHeA8wFLpY0t7hVpaYH+LOImAucCnwi2ddrgAcjYjbwYDI/2nwSeC5v/qvAP0bEkcBm4LKiVJWurwP/FRFHA28it/+j9reWNAW4GmiKiOPIPWLgIkbnb/0dYP6Atr39tu8h9zyd2eQez/zPQ/mikg4I4GRgRUSsjIgu4C7g/CLXlIqIWBsRTyXT28j9wZhCbn//LVnt34ALilJgSiRNBc4FvpXMCzgLuDtZZTTu81jgDHLPWyEiuiJiC6P8tyb3fJvq5OmUNcBaRuFvHREPk3t+Tr69/bbnA3dEzmPAIZIOG+x3lXpATAFW5823JG2jmqQZwDzgceDQiFibLFoHHFqsulJyE/BZoC+ZbwC2RERPMj8af/OZQCvwr8nQ2rck1TKKf+uIWAP8PfAyuWDYCjzJ6P+t++3tt92vv3GlHhAlR1IdcA/wqYhoy1+WPO511Jz3LOl9wPqIeLLYtRxgZcCbgX+OiHnADgYMJ43C33ocuX8tzwQmA7W8dhimJAznb1vqAbEGmJY3PzVpG5UklZMLh+9FxI+S5lf7u5zJ+/pi1ZeC04HzJK0iN3x4Frmx+UOSYQgYnb95C9ASEY8n83eTC4zR/FufDbwYEa0R0Q38iNzvP9p/6357+233629cqQfEImB2cqZDBbmDWguKXFMqkrH3bwPPRcTX8hYtAC5Jpi8B/u+Bri0tEXFtREyNiBnkfttfRMSHgYXAhclqo2qfASJiHbBa0pyk6XfIPd991P7W5IaWTpVUk/y33r/Po/q3zrO333YB8NHkbKZTga15Q1FvqOSvpJb0XnLj1Fng9oi4obgVpUPS24BfAUvYPR7/eXLHIX4IHE7udum/HxEDD4Ad9CSdCXwmIt4n6QhyPYrxwNPARyKis4jlDTtJJ5I7MF8BrAQ+Ru4fhKP2t5b0ZeCD5M7Yexr4OLnx9lH1W0v6PnAmudt6vwp8CfgxBX7bJCz/idxwWzvwsYhoHvR3lXpAmJlZYaU+xGRmZnvhgDAzs4IcEGZmVpADwszMCnJAmJlZQQ4IsyGQ1Ctpcd5r2G54J2lG/h06zYqt7I1XMbM8OyPixGIXYXYguAdhNgwkrZJ0o6Qlkp6QdGTSPkPSL5J78T8o6fCk/VBJ90r6dfJ6a/JRWUn/kjzX4GeSqou2U1byHBBmQ1M9YIjpg3nLtkbE8eSuXL0pafsG8G8RcQLwPeDmpP1m4KGIeBO5+yQtTdpnA7dExLHAFuD3Ut0bs9fhK6nNhkDS9oioK9C+CjgrIlYmN0VcFxENkjYAh0VEd9K+NiImSGoFpubf9iG5DfvPk4e+IOlzQHlE/PUB2DWz13APwmz4xF6mhyL/PkG9+DihFZEDwmz4fDDv/X+S6UfJ3UkW4MPkbpgIucdCXgm7npk99kAVaTZY/teJ2dBUS1qcN/9fEdF/qus4Sc+Q6wVcnLT9Cbknu/05uae8fSxp/yRwm6TLyPUUriT3JDSzEcPHIMyGQXIMoikiNhS7FrPh4iEmMzMryD0IMzMryD0IMzMryAFhZmYFOSDMzKwgB4SZmRXkgDAzs4L+P84xDxwRVvBAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.1)\n",
    "tf.model.compile(loss='mse', optimizer=sgd)\n",
    "\n",
    "tf.model.summary()\n",
    "\n",
    "# fit() trains the model and returns history of train\n",
    "history = tf.model.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "y_predict = tf.model.predict(np.array([5, 4]))\n",
    "print(y_predict)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 08 : multi-variable linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복습 recap\n",
    "\n",
    "1. Hypothesis : 우리의 가설, 우리의 예측을 어떻게 할 것인지.\n",
    "\n",
    "2. Cost function : 우리가 잘 예측했는지, cost가 얼마나 나오는지 확인\n",
    "\n",
    "3. Gradient descent algorithm : cost 함수를 최적화하는 알고리즘."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input 하나가 아니라 여러개라면? 어떻게 할까?\n",
    "\n",
    "quiz1 quiz2 quiz3의 변수(input)을 가지고 y값을 예측할 수 있을까?\n",
    "\n",
    "다변수일 때, 간단하게 표현하는 Hypothesis\n",
    "\n",
    "$H(x_1, x_2, x_3) = w_1x_1 + w_2x_2 + w_3x_3 + b$\n",
    "\n",
    "$H(x_1, x_2, x_3, ... , x_n) = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b$ // 이와같이 다변수여도 그냥 변수마다 가중치 곱해주고 + bias\n",
    "\n",
    "$cost(W, b) = \\frac{1}{m}\\sum_{i=1}^m(H(x_1^{(i)}, x_2^{(i)}, x_3^{(i)})-y^{(i)})^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조금 불편한 것이 생긴다.\n",
    "\n",
    "많아지면 많아질수록 옆으로 길게 곱을 계속해야 하는가? 잘 처리하는 방법이 없나?\n",
    "\n",
    "__Matrix__ \n",
    "\n",
    "Matrix multiplication(매트릭스의 곱셈) : 매트릭스의 곱셈을 표현하여 우리가 하고싶은 가설을 표현한다.\n",
    "\n",
    "$(x_1 x_2 x_3)\\cdot$ $ \\begin{pmatrix}\n",
    "w_1\\\\ \n",
    "w_2\\\\\n",
    "w_3\n",
    "\\end{pmatrix}$ $= (x_1w_1 + x_2w_2 + x_3w_3)$\n",
    "\n",
    "우리에게 필요한 긴 값을 매트릭스로 표현하면 짧게 표현할 수 있다.\n",
    "\n",
    "wx 와 xw는 같지 사실. 그러나 매트릭스이기 때문에 표현해주면\n",
    "\n",
    "그래서 매트릭스를 쓸 때에는 $H(X) = XW$ 라고 표현한다. 또한 대문자는 매트릭스의 표현이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 데이터에 표현하기 - Many instances\n",
    "\n",
    "실제 데이터의 경우에는 데이터 instance가 많다. 세로로의 갯수를 말하는 것. 한 데이터 세트 X_train 과 Y의 세트를 instance라고 함.\n",
    "\n",
    "물론 instance 각각을 계속 넣어서 weight 를 구하는 방법도 있다. 그러나 효율성이 떨어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매트릭스의 놀라운점. Hypothesis using matrix\n",
    "\n",
    "instance만 늘어나고 weight 의 갯수는 그대로 해도 매트릭스의 곱셈을 이용하면 원하는 것을 다 얻을 수 있다.\n",
    "\n",
    "$ \\begin{pmatrix}\n",
    "x_{11} x_{12} x_{13}\\\\ \n",
    "x_{21} x_{22} x_{23}\\\\\n",
    "x_{31} x_{32} x_{33}\\\\\n",
    "x_{41} x_{42} x_{43}\\\\\n",
    "x_{51} x_{52} x_{53}\n",
    "\\end{pmatrix} \\cdot$ $\\begin{pmatrix}\n",
    "w_1\\\\ \n",
    "w_2\\\\\n",
    "w_3\n",
    "\\end{pmatrix}$ $ = \\begin{pmatrix}\n",
    "x_{11}w_1+ x_{12}w_2+ x_{13}w_3\\\\ \n",
    "x_{21}w_1+ x_{22}w_2+ x_{23}w_3\\\\\n",
    "x_{31}w_1+ x_{32}w_2+ x_{33}w_3\\\\\n",
    "x_{41}w_1+ x_{42}w_2+ x_{43}w_3\\\\\n",
    "x_{51}w_1+ x_{52}w_2+ x_{53}w_3\n",
    "\\end{pmatrix}$\n",
    "\n",
    "[5, 3] * [3, 1] = [5, 1]\n",
    "\n",
    "$H(X) = XW$\n",
    "\n",
    "매트릭스의 또 다른 장점은 인스턴스가 많을 때, 인스턴스의 전체를 매트릭스에 넣고 w를 곱해버리면 우리가 원하는 값을 얻음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종종하는 매트릭스 계산\n",
    "\n",
    "$H(X) = XW$\n",
    "\n",
    "여기서 X는 데이터에 따라 나오는 것이기 때문에 종종 주어진다.\n",
    "    - ex [5, 3]\n",
    "    - 여기서 5는 instance의 개수\n",
    "    - 여기서 3은 variable 변수의 개수\n",
    "\n",
    "여기서 H(X) 출력값도 종종 주어진다. (특히 linear regression 경우에는 출력값이 하나만 나오기 때문에)\n",
    "    - ex [5, 1]\n",
    "   \n",
    "__우리가 설계해야 하는 것, W에 해당하는 weight의 크기를 얼마로 할 것인가.__\n",
    "\n",
    "X 즉 데이터의 변수 개수를 가져온다. ex 3\n",
    "\n",
    "H(x) 결과값의 개수를 가져온다. ex 1\n",
    "\n",
    "그러면 W 매트릭스는 [3, 1] 이다. X 변수 개수와 결과값의 형태 개수가 W 매트릭스의 형태."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instance가 여러개 n개 일때\n",
    "\n",
    "X W = H(X)\n",
    "\n",
    "[n, 3] [3, 1] [n, 1]\n",
    "\n",
    "[None, 3] [3, 1][None, 1] 텐서플로우에서는 양의 정수를 None으로 표시."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력이 하나가 아닐 때에도 쓰기 좋음.\n",
    "\n",
    "입력 : n개의 입력과 3개의 변수가 있을 때 = [n, 3]\n",
    "\n",
    "출력 : 2개의 출력을 원한다. = [n, 2]\n",
    "\n",
    "W : [3, 2]\n",
    "\n",
    "__매트릭스의 경우에는 멀티 변수의 경우와 다 인스턴스 경우와 다 출력 경우 모두 해결할 수 있다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WX vs XW\n",
    "\n",
    "이론에서 렉쳐에서는 $H(x)=Wx+b$ 라고 쓰고\n",
    "\n",
    "현실에서, 실무에서는 $H(X) = XW$ 이렇게 쓴다.\n",
    "\n",
    "그러나 둘의 차이는 매트릭스 이고 아니고 이다.(다변수 일때 매트릭스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
