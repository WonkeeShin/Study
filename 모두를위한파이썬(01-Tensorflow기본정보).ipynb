{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 01 : 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Andrew Ng's ML class\n",
    "  1. https://class.coursera.org/ml-003/lecture (앤드류 응 교수님꺼 무조건 들으셈)\n",
    "  2. http://www.holehouse.org/mlclass/ (앤드류 응 교수님 수업 필기본)\n",
    "\n",
    "2. Convolutional Neural Networks for Visual Recognition.\n",
    "  1. http://cs23ln.github.io/\n",
    "\n",
    "3. Tensorflow\n",
    "    1. https://www.tensorflow.org\n",
    "    2. https://github.com/aymericdamien/TensorFlow-Examples (텐서 예시들)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 02 : ML 이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning 은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__배경 :__ Limitations of explicit programming : 특정한 조건에 어떤 행동을하라고 구체적인 지시를 일일이 하는 것에는 한계가 있다.\n",
    "\n",
    "ex) 스팸 필터(조건 너무 많음), 자동운전시스템(조건많)\n",
    "\n",
    "__탄생 :__\n",
    "Arthur Samuel (1959) : 머신러닝은 개발자의 그런 구체적인 명령없이 컴퓨터가 스스로 배울 수 있는 능력을 갖는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised/Unsupervise learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning :__ label 이 정해진, 답이 있는 예시들을 가지고 배우는 것\n",
    "\n",
    "ML에서 대부분의 문제 유형들은 Supervised learning 이다.\n",
    "\n",
    "ex : 이미지 라벨링, 스팸메일 구분, 점수 예측.\n",
    "\n",
    "__Types of supervised learning :__ 지도 학습의 종류\n",
    "1. Regression (회귀) ; 실수값을 예측하는 것. 변수 값들에 맞는 곡선이나 직선을 예측하는 것. 오차를 최소한으로 함.\n",
    "    ex) 공부시간에 따른 점수 분포\n",
    "2. Binary classification (이분법 분류) : 딱 두 가지의 종류로 분류하는 것\n",
    "3. Multi-label classification (다중 라벨 분류) ; 여러 가지의 라벨로 분류하는 것.\n",
    "\n",
    "__Unsupervised learning :__ label 이 없고, grouping, clustering 같이 묶는 것 등.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 03 : TensorFlow의 설치 및 기본적인 operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor : 데이터 중에서 3차원 데이터.\n",
    "\n",
    "Data Flow Graph : 노드와 엣지로 연결된 그래프. 데이터는 엣지를 통해 흐른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 operations (TF version 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constant, Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tf.constant(\" \") :__ 텐서플로우에서 쓰는 변수. 해당 값의 노드를 추가한다.\n",
    "\n",
    "__tf.Session() :__ 텐서플로우 1 에서는 session을 만들고 거기에서 run을 하여 print 할 수 있었다.\n",
    "\n",
    "__print(sess.run(변수)) :__ 해당 constant 노드를 실행하여 print 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello, TensorFlow!', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF 1 버전\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node add 노드 더하기\n",
    "\n",
    "node = tf.add(node1, node2)\n",
    "\n",
    "node = node1 + node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.3, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(1.0, tf.float32)\n",
    "node2 = tf.constant(2.3, tf.float32)\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.3, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node4 = node1 + node2\n",
    "\n",
    "print(node4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF 1 버전\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder 노드 : 지정해주는 값을 변수로 가짐.\n",
    "\n",
    "TF2 에서는 placeholder 가 없고 그냥 함수를 정의한다.\n",
    "\n",
    "tf.placeholder( data type ) : run 할 당시에 들어오는 값을 노드로 만든다.\n",
    "\n",
    "feed_dict 가 들어오는 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def adder(a, b):\n",
    "    return a + b\n",
    "    \n",
    "A = tf.constant(2)\n",
    "B = tf.constant(3)\n",
    "\n",
    "print(adder(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF1  example\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_noder = a + b\n",
    "sess.run(adder_node, feed_dict={a: 3, b: 4.5})\n",
    "sess.run(adder_node, feed_dict={a: [1, 3], b: [2, 4]})) #이런 식으로 array 더하기 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 기본정보 (Tensor Ranks, Shapes, and Types)\n",
    "\n",
    "__Tensor Ranks :__ 몇 차원 array냐를 물어보는 것.\n",
    "\n",
    "|랭크|이름|예시|\n",
    "|:-|-|:-|\n",
    "|0랭크|스칼라|(값만 있음) ex 483|\n",
    "|1랭크|벡터|(값과 방향) ex [1, 2, 3]|\n",
    "|2랭크|매트릭스|(수의 표) ex [[1, 2, 3], [2, 3, 4], [5, 6, 7]]|\n",
    "|3랭크|3-Tensor|(수의 큐브) ex [[[1], [2], [3]], [[3], [4], [5]], [[5], [5], [7]]]|\n",
    "|n랭크|n-tensor|ex|\n",
    "\n",
    "\n",
    "__Tensor Shapes :__ 각각의 엘리먼트에 몇 개씩 들어있느냐.\n",
    "\n",
    "|Rank |Shape |Dimension number|\n",
    "|-|-|-|\n",
    "|0랭크 |[]| 0-D|\n",
    "|1랭크 |[D0]| 1-D|\n",
    "|2랭크 |[D0, D1]| 2-D|\n",
    "|3랭크 |[D0, D1, D2]| 3-D|\n",
    "|n랭크 |[D0, D1, D2 ... ]|\n",
    "\n",
    "\n",
    "__Tensor Types__\n",
    "\n",
    "주로 tf.float32 와 tf.int32를 많이 쓴다.\n",
    "\n",
    "\n",
    "__TensorFlow Mechanics__\n",
    "1. Build graph using TensorFlow operations : 텐서플로우 그래프 빌드하기\n",
    "2. sess.run (op) : 그래프 돌리기\n",
    "3. 그래프 변수 업데이트."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 04 : Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear Regression : 주어진 데이터들을 학습하여 실수값을 예측할 수 있는 직선 모델.\n",
    "\n",
    "기본적으로 많은 자연 현상들은 직선을 띈다.\n",
    "\n",
    "__학습 :__ 데이터에 맞는 선(모델)을 찾는 것.\n",
    "\n",
    "__Linear :__ 즉 직선형 모델. $H(x)=Wx+b$\n",
    "\n",
    "*Hypothesis 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떤 모델(가설)이 최적일까?\n",
    "\n",
    "기본적으로 원래 데이터와 예측한 값과의 거리가 멀수록 안좋은 모델이다.\n",
    "\n",
    "__Cost function(Loss function) :__ 예측 값과 원래 데이터와의 거리를 구하는 함수\n",
    "\n",
    "__학습의 목적 :__ 즉 cost 함수를 최소화하는 것. 최적화하는 것이 학습의 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function(loss function) :\n",
    "\n",
    "cost function(loss function) : 예측값과 원래 y 간의 거리 구하는 함수.\n",
    "\n",
    "가장 기본적인 cost function : $H(x)-y$  // 절댓값 처리가 안되어 있음.\n",
    "\n",
    "__가장 자주쓰는 cost function :__ $(H(x)-y)^2$  // 절댓값 처리도 되고, 거리가 멀수록 페널티 줄 수 있음. 오차제곱의 평균\n",
    "\n",
    "cost function : $cost=\\frac{1}{m}\\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^{2}$ 일반적인 데이터에서 cost 함수\n",
    "\n",
    "*m:총 데이터 개수.\n",
    "\n",
    "__cost function(W, b) :__ $cost(W, b)=\\frac{1}{m}\\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^{2}$ cost 함수, 손실 함수는 W, b에 대한 함수가 된다.\n",
    "\n",
    "*$H(x)=Wx+b$이므로 W와 b값이 변화함에 따라서 cost 함수 값이 변화함.\n",
    "\n",
    "__즉 학습의 목적은 cost(W, b)가 가장 작게 나올 때의 W와 b를 구하는 것.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 05 : Linear Regression Tensorflow로 구현 (실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적 : x와 y는 이미 정해져 있다. 정해진 X y에 해당하는 최적의 W b를 찾는 것.\n",
    "\n",
    "1. W, b를 랜덤으로 일단 변수로 잡는다.\n",
    "2. hypothesis = X * W + b\n",
    "3. cost function 을 오차제곱의 평균으로 잡는다.\n",
    "4. cost function 을 줄이는 최적화 방법 : 경사하강법. (learning_rate 설정)\n",
    "    - 여기서 W, b가 계속 업데이트 된다.\n",
    "5. train 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 2 Linear Regression\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(2001):\n",
    "        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val, b_val)\n",
    "\n",
    "'''\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]\n",
    "\"\"\"\n",
    "0 2.82329 [ 2.12867713] [-0.85235667]\n",
    "20 0.190351 [ 1.53392804] [-1.05059612]\n",
    "40 0.151357 [ 1.45725465] [-1.02391243]\n",
    "...\n",
    "1920 1.77484e-05 [ 1.00489295] [-0.01112291]\n",
    "1940 1.61197e-05 [ 1.00466311] [-0.01060018]\n",
    "1960 1.46397e-05 [ 1.004444] [-0.01010205]\n",
    "1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n",
    "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Verision 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sequential()__\n",
    "\n",
    "ex : tf.keras.Sequential()\n",
    "\n",
    "각각 레이어에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택일 때 적합하다.\n",
    "\n",
    "선형 모델, 순차 모델. 들어오는 방향이 한 군데, 나가는 방향도 한 군데.\n",
    "\n",
    "모델의 큰 형태. templete 느낌. 껍데기고, 내부를 채워야 함.\n",
    "\n",
    "input_shape를 모르면 weight 값이 없어서 summary를 볼 수가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dense()__\n",
    "\n",
    "기본적으로 hidden layer 만들 수 있는 layer. 노드 수를 정해줄 수 있다.\n",
    "\n",
    "output : input과 가중치값을 dot하고 + bias를 하여 활성화함수에 넣는다.\n",
    "\n",
    "input rank 가 2보다 크다면 input 과 kernel(가중치)는 dot 계산을 한다.\n",
    "\n",
    "__입력 형태정보 :__ Sequential 모델의 첫번째 레이어는 입력형태 정보 필요.\n",
    "\n",
    "__Dense :__ 2D 레이어 이기 때문에, 입력 형태를 input_dim 으로 넣어줄 수 있다.\n",
    "\n",
    "__input shape ;__ input_shape=(None, 10) 등 정수와 None으로 이루어진 튜플.\n",
    "\n",
    "나머지 레이어들은 input_shape=(12, 2) 등의 튜플 형태로 써야한다. None은 음이 아닌 어떠한 정수를 받을 수 있음.\n",
    "\n",
    "ex : tf.keras.layers.Dense(units=1, input_dim=1)) \n",
    "    - units : output shape\n",
    "    - input_dim : input shape\n",
    "    \n",
    "ex : tf.keras.layers.Dense(10, input_shape=(1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__compile()__ : 학습시키기 전, 학습 방식에 대한 환경설정.\n",
    "\n",
    "1. optimizer(정규화기) : SGD 같은 것.\n",
    "2. loss function(손실 함수) : mse 같은 것. mean_squared_error , $\\frac{1}{m} * \\sum(y'-y)^2$\n",
    "3. metric (기준) : metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.7035\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2692\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6219\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3282\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1935\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1303\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0734\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0670\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0621\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0580\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0544\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0511\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0481\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0452\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0425\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0400\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0377\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0354\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0333\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0314\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0246\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0099\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0082\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0065\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0057\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0054\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0048\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0042\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0035\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0028\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0022\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0017\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0014\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0010\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.8106e-04\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2319e-04\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6874e-04\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1750e-04\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6928e-04\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2391e-04\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.8121e-04\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.4103e-04\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0322e-04\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6764e-04\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3416e-04\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.0266e-04\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7301e-04\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4511e-04\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1886e-04\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9415e-04\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7090e-04\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4903e-04\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 3.2844e-04\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.0907e-04\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9084e-04\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.7368e-04\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5754e-04\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.4235e-04\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2806e-04\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1461e-04\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0195e-04\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9004e-04\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7883e-04\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6828e-04\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5836e-04\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.4902e-04\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4023e-04\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3196e-04\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2417e-04\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1685e-04\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0996e-04\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0347e-04\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7368e-05\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1625e-05\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6220e-05\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1136e-05\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6350e-05\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.1847e-05\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7610e-05\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3621e-05\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9869e-05\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.6337e-05\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3015e-05\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.9888e-05\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6945e-05\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4176e-05\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1571e-05\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9119e-05\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.6812e-05\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4640e-05\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.2598e-05\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0675e-05\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8865e-05\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7163e-05\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5561e-05\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4054e-05\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.2634e-05\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1300e-05\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0043e-05\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8861e-05\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7749e-05\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6702e-05\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5717e-05\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4790e-05\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3918e-05\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3097e-05\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2324e-05\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1597e-05\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0913e-05\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.0269e-05\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6639e-06\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.0939e-06\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5576e-06\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 8.0526e-06\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5780e-06\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.1308e-06\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7102e-06\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.3146e-06\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9420e-06\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.5915e-06\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2618e-06\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9512e-06\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6593e-06\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3844e-06\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.1259e-06\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8826e-06\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6537e-06\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4381e-06\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2352e-06\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0445e-06\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8649e-06\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6959e-06\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5370e-06\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.3873e-06\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.2465e-06\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1139e-06\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9894e-06\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8720e-06\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7615e-06\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6577e-06\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5599e-06\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4678e-06\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.3814e-06\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2999e-06\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.2232e-06\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.1511e-06\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0832e-06\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.0193e-06\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 9.5915e-07\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0262e-07\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 8.4933e-07\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9930e-07\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5206e-07\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.0779e-07\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6596e-07\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2672e-07\n",
      "[[-3.9986835]\n",
      " [-2.9993227]]\n"
     ]
    }
   ],
   "source": [
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# units == output shape, input_dim == input shape\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.1)  # SGD == standard gradient descendent, lr == learning rate\n",
    "tf.model.compile(loss='mse', optimizer=sgd)  # mse == mean_squared_error, 1/m * sig (y'-y)^2\n",
    "\n",
    "# prints summary of the model to the terminal\n",
    "tf.model.summary()\n",
    "\n",
    "# fit() executes training\n",
    "tf.model.fit(x_train, y_train, epochs=200)\n",
    "\n",
    "\n",
    "# predict() returns predicted value\n",
    "y_predict = tf.model.predict(np.array([5, 4]))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강의 06 : SGD 원리 (Standard Gradient Descendent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설 간단화 Simplified hypothesis\n",
    "\n",
    "$H(x) = Wx$ 계산을 간단히 보여주기 위해서 bias 를 없앤다. 그렇게 되면 cost function은\n",
    "\n",
    "$cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$\n",
    "\n",
    "cost(W)함수는 W의 변화에 따른 Cost 값을 보여준다.\n",
    "\n",
    "이 함수는 이차함수 형태를 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm (경사하강법)  : cost 함수를 최소화하자.\n",
    "\n",
    "- loss 함수를 최소화하다\n",
    "- 대부분의 최소화 문제에서 경사 하강법을 사용한다\n",
    "- cost(loss)를 최소화 하기위한 W, b를 찾는다.\n",
    "- 실생활에서는 함수의 극값을 찾을 수 없기 때문에 사용하는 방법.\n",
    "\n",
    "__작동원리__\n",
    "\n",
    "- 랜덤 초기값( 혹은 특정한 초기값 )에서 시작한다.\n",
    "- W, b 에 따른 경사값이 0이 되는 방향으로 계속 진행한다\n",
    "- 어디서 시작하는지가 어느 최솟값으로 끝나는지를 결정한다.\n",
    "\n",
    "__Convex function__\n",
    "\n",
    "- SGD는 가끔 시작점의 변화(W, b)에 따라 최솟값이 아닌 local 최솟값에 수렴할 수 있다. (제대로 동작 X)\n",
    "- 우리의 현재 cost(W, b) 함수는 밥그릇 뒤집어 놓은 Convex function 이다.\n",
    "- Convex function의 경우 SGD를 사용하면 무조건 답을 찾을 수 있다.\n",
    "- Convex function 인지 아닌지 확인 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 수식 표현\n",
    "\n",
    "1. $cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ 를 2로 나눠 계산을 편하게 한다. (결과에는 지장 없음)\n",
    "2. $cost(W) = \\frac{1}{2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ cost함수를 W에 대해 편미분 한다.\n",
    "3. $W :=W - \\alpha\\frac{\\partial}{\\partial W}cost(W)$ W에 대한 cost 함수 이므로, W에 대해 편미분한다. 그리고 W를 업데이트한다.\n",
    "4. $W :=W - \\alpha\\frac{\\partial}{\\partial W}\\frac{1}{2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$ 편미분하는 과정\n",
    "5. $W :=W - \\alpha\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$ W에 대한 곱미분이다.\n",
    "\n",
    "SGD 정리 : $W :=W - \\alpha\\frac{\\partial}{\\partial W}\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$\n",
    "\n",
    "여기서 $\\alpha$는 learning rate 이다. 얼마나 학습할 것인지를 정한다. 현재 기울기를 얼마나 반영하여 뺄 것인가.\n",
    "\n",
    "여기서 $\\frac{1}{m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$ 이 부분은 현재 W에 의한 cost 값의 기울기이다.\n",
    "\n",
    "따라서 현재 기울기가 양수라면 값이 줄어들어 0에 가까워질 것이고, 현재 기울기가 음수라면 값이 늘어가 0에 가까워질 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
